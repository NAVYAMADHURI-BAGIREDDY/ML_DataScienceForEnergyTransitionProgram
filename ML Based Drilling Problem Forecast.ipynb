{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drilling problem forecast\n",
    "**Project core idea**\n",
    "\n",
    "\"It is easier to land on man on the moon than describing and penetrating a petroleum reservoir successfully\" Wrote Pr. Farouq Ali, Indeed, the petroleum engineering is one of the few branches with the lack of the luxurious aspect\n",
    "of visual during problem solving, and hence tackling the later requires nothing less than approaching it through a diverse set of interconvoluted disciplines.\n",
    "\n",
    "In fact even if we plan very carefully, it is almost certain that problems related to drilling operations will happen while operating. The slightest of\n",
    "underestimations could lead to fatal results, ranging from millions of dollars loss in company assets to more critically human lives. Hence understanding and anticipating drilling problems, understanding their causes, and planning solutions are necessary for an overall well cost control which ensures successfully reaching the target zone.\n",
    "\n",
    "**Problem types**\n",
    "* Lost cercultion\n",
    "* Stuck pipe\n",
    "* Equipement failure\n",
    "\n",
    "**Data source**\n",
    "* Treated Dailly Drilling Report and End Well Reports\n",
    "* Extracted well logs data after digitalization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treated data source\n",
    "Check the note book in EDM_DDR/Extracted_CSV_data/Well_events/Treatment_for_Geo_Drill_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Loading data and libreries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>Libraries</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # read and wrangle dataframes\n",
    "import itertools\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# data visualization\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "import matplotlib.patches as pathes \n",
    "import plotly.graph_objects as go \n",
    "from plotly.subplots import make_subplots \n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "\n",
    "from sklearn.base import TransformerMixin # To create new classes for transformations\n",
    "from sklearn.preprocessing import (FunctionTransformer, StandardScaler) # preprocessing \n",
    "from sklearn.decomposition import PCA # dimensionality reduction\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy.stats import boxcox # data transform\n",
    "from sklearn.model_selection import (train_test_split, KFold , StratifiedKFold, \n",
    "                                     cross_val_score, GridSearchCV, RandomizedSearchCV,\n",
    "                                     learning_curve, validation_curve) # model selection modules\n",
    "from sklearn.pipeline import Pipeline # streaming pipelines\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # To create a box-cox transformation class\n",
    "from collections import Counter\n",
    "import warnings\n",
    "# load models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import (XGBClassifier, plot_importance)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Importing the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Deep learning\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Save the model\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Data treatment\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "# For Notebooks\n",
    "init_notebook_mode(connected=True)\n",
    "# For offline use\n",
    "cf.go_offline()\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "\n",
    "np.random.seed(0)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>load the dataset</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Files \n",
    "Csv_file_list = []\n",
    "path = \"C:/Users/DELL/Desktop/Volve dump base data xml/Web_app_models/Problem forecast/Geo_log_prob_NW_data/Spec_data/\"\n",
    "# View contents of the path\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Csv_file_list=[]\n",
    "for file in files:\n",
    "    if file.lower().endswith('.csv'):\n",
    "        Csv_file_list.append(path + file)\n",
    "        \n",
    "Csv_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two data set one as a seperate list\n",
    "# The second is a concatenated version\n",
    "df_list=[]\n",
    "for file in Csv_file_list:\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    df = pd.read_csv(file, index_col=None)\n",
    "    df_list.append(df)\n",
    "    \n",
    "for df in df_list:\n",
    "    df.loc[df.OS == 0, 'OS_cat'] = \"Normal condition\"\n",
    "    df.loc[df.OS == 1, 'OS_cat'] = \"Lost circulation\"\n",
    "    df.loc[df.OS == 2, 'OS_cat'] = \"Stuck equipment\"\n",
    "    df.loc[df.OS == 3, 'OS_cat'] = \"Equipment failure\"\n",
    "    \n",
    "well1, well2, well3, well4, well5=df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "        print(f'''Example of the dataset {files[i][:-4]}''')\n",
    "        display(df_list[i].describe())\n",
    "        display(df_list[i].head())\n",
    "        display(df_list[i].info())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drilling_data=[]\n",
    "#Drilling_data = pd.concat(df_list)\n",
    "#df=Drilling_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All_columns=[]\n",
    "#for df in df_list:\n",
    "#    col=list(df.columns)\n",
    "#    All_columns.append(col)\n",
    "\n",
    "    \n",
    "#for ele in CC:\n",
    "#    if ele in All_columns:\n",
    "#        All_columns.remove(ele)\n",
    "\n",
    "# Not in commune values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation for list of dataframes\n",
    "num_dfs=[]\n",
    "for df in df_list:\n",
    "    num_df=df.select_dtypes(include=np.number)\n",
    "    num_dfs.append(num_df)\n",
    "    \n",
    "cat_dfs=[]\n",
    "for df in df_list:\n",
    "    cat_df=df.select_dtypes(include=np.number)\n",
    "    cat_dfs.append(cat_df)\n",
    "    \n",
    "\n",
    "num_data=df_list[0].select_dtypes(include=np.number)\n",
    "num_cols=num_data.columns.tolist()\n",
    "\n",
    "cat_data=df_list[0].select_dtypes(exclude=np.number)\n",
    "cat_cols=cat_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data couvrage\n",
    "for df in df_list:\n",
    "    msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = LabelEncoder()\n",
    "#for df in df_list: \n",
    "#    for cat in cat_cols:\n",
    "#        df[cat] = label.fit_transform(df[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Backup_1= df_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_wells= pd.concat(df_list)\n",
    "All_wells['wellName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.remove('wellName')\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "for cat in cat_cols:\n",
    "    All_wells[cat]=label.fit_transform(All_wells[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_wells.reset_index(inplace=True)\n",
    "All_wells.drop('index', axis=1, inplace=True)\n",
    "All_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_wells.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well1= All_wells.loc[All_wells['wellName'] == \"15/9-F-4\"]\n",
    "well2= All_wells.loc[All_wells['wellName'] == \"15/9-F-15\"]\n",
    "well3= All_wells.loc[All_wells['wellName'] == \"15/9-F-1\"]\n",
    "well4= All_wells.loc[All_wells['wellName'] == \"15/9-19_SR\"]\n",
    "well5= All_wells.loc[All_wells['wellName'] == \"15/9-F-10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[well1, well2, well3, well4, well5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df.loc[df.GR > 200, 'GR'] = 200\n",
    "    df.Resistivity=np.log(df.Resistivity)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>Predicting using Well logs only</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df.rename({'DEPT':'MD'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use 4 well for taining and one for prediction\n",
    "# train columns to use\n",
    "well_df=df_list.copy()\n",
    "\n",
    "train_cols =['MD','Azim', 'INCL','GR', 'Resistivity', 'DT', 'DTS', 'RHOB','OS']\n",
    "\n",
    "# test columns to use\n",
    "test_cols =[ 'MD', 'Azim', 'INCL','GR', 'Resistivity', 'DT', 'DTS', 'RHOB' ]\n",
    "well_df_restrict = [0] * 5\n",
    "\n",
    "for i in range(len(well_df)):\n",
    "    if i <= 3:\n",
    "        # the train data, drop unwanted columns\n",
    "        well_df_restrict[i] = well_df[i][train_cols]\n",
    "  \n",
    "    else:\n",
    "        # the test data, drop unwanted columns\n",
    "        well_df_restrict[i] = well_df[i][test_cols]\n",
    "\n",
    "well1, well2, well3, well4, well5 = well_df_restrict\n",
    "\n",
    "# as we can see, both ends already not have NaNs\n",
    "well5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well5.to_csv(\"C:/Users/DELL/Desktop/data examples/Problem_forecast_example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the test data\n",
    "well = well1\n",
    "\n",
    "# define what data we wanna plot are we going to us\n",
    "logs =['Azim', 'INCL','GR', 'Resistivity', 'DT', 'DTS', 'RHOB']\n",
    "\n",
    "# create the subplots; ncols equals the number of logs\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(logs), figsize=(13,10))\n",
    "\n",
    "# looping each log to display in the subplots\n",
    "\n",
    "colors = ['black', 'red', 'blue', 'green', 'purple', 'black','brown']\n",
    "\n",
    "for i in range(len(logs)):\n",
    "    ax[i].plot(well[logs[i]], well.MD, color=colors[i])\n",
    "\n",
    "    ax[i].set_title(logs[i])\n",
    "    ax[i].grid(True)\n",
    "    ax[i].invert_yaxis()\n",
    "\n",
    "plt.tight_layout(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting each similar feature togheter to analyze it's distribution\n",
    "\n",
    "'''15/9-F-15_final    53039\n",
    "15/9-F-15 A        51708\n",
    "15/9-F-14          47643\n",
    "15/9-F-5           18548\n",
    "15/9-F-9 A         13746\n",
    "15/9-F-9            7851\n",
    "15/9-F-7            6389'''\n",
    "\n",
    "box_data=df_list.copy()\n",
    "\n",
    "for df in box_data:\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "data=[]\n",
    "def Feature_all(feature):\n",
    "    df=pd.DataFrame()\n",
    "\n",
    "    for i in range(len(box_data)):\n",
    "                for j in box_data[i].columns:\n",
    "                    if j==str(feature):\n",
    "                        df[i]=box_data[i][j]\n",
    "                        #data.append(df[i])\n",
    "    df.boxplot(figsize=(12,7))\n",
    "    plt.xticks([1, 2, 3, 4, 5], ['15_9-19_SR', '15_9-F-1', '15_9-F-10', '15_9-F-15', '15_9-F-4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_all('GR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data treatment\n",
    "### a- Data Normalization\n",
    "\n",
    "Normalize the dataset:\n",
    "* Log transform the RT log\n",
    "* Use power transform with Yeo-Johnson method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train = pd.concat([well1, well2, well3, well4])\n",
    "well_pred = pd.concat([well5])\n",
    "\n",
    "well4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "well_train=[]\n",
    "well_pred=[]\n",
    "\n",
    "well_train = pd.concat([well1, well2, well3, well4])\n",
    "well_pred = pd.concat([well5]) # 15_9-F-4\n",
    "\n",
    "\n",
    "# assign names\n",
    "\n",
    "names= ['15_9-19_SR', '15_9-F-1', '15_9-F-10', '15_9-F-15', '15_9-F-4']\n",
    "names_train = []\n",
    "names_pred = []\n",
    "for i in range(len(well_df_restrict)):\n",
    "    if i  <= 3:\n",
    "        # train data, assign names \n",
    "        _ = np.full(len(well_df_restrict[i]), names[i])\n",
    "        names_train.append(_)\n",
    "    else:\n",
    "        # test data, assign names\n",
    "        _ = np.full(len(well_df_restrict[i]), names[i])\n",
    "        names_pred.append(_)\n",
    "\n",
    "# concatenate inside list\n",
    "import itertools\n",
    "\n",
    "names_train = list(itertools.chain.from_iterable(names_train))\n",
    "names_pred = list(itertools.chain.from_iterable(names_pred))\n",
    "\n",
    "# include well names to the train and pred dataframe\n",
    "well_train['WELL'] = names_train\n",
    "well_pred['WELL'] = names_pred\n",
    "\n",
    "# move the depth column to the right\n",
    "depth_train, depth_pred = well_train.pop('MD'), well_pred.pop('MD')\n",
    "well_train['MD'], well_pred['MD'] = depth_train, depth_pred\n",
    "\n",
    "well_train['OS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = ['Azim', 'INCL','GR', 'Resistivity', 'DT', 'DTS', 'RHOB', 'OS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(well_train, vars=train_features, diag_kind='kde',\n",
    "             plot_kws = {'alpha': 0.6, 's': 30, 'edgecolor': 'k'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_only_features = well_train[train_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(well_train_only_features.corr(method = 'spearman') , dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.cubehelix_palette(n_colors=12, start=-2.25, rot=-1.3, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(well_train_only_features.corr(method = 'spearman') ,annot=True,  mask=mask, cmap=cmap, vmax=.3, square=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data treatment\n",
    "### a- Data Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = well_train.columns\n",
    "only_feature = ['Azim', 'INCL','GR', 'Resistivity', 'DT', 'DTS', 'RHOB']# only feature column names\n",
    "only_target = 'OS' # only target column names\n",
    "feature_target = np.append(only_feature, only_target) # feature and target column names\n",
    "\n",
    "colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train['OS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler=StandardScaler()\n",
    "#column_drop = ['WELL', 'Depth m']\n",
    "\n",
    "## fit and transform\n",
    "#well_train_norm = scaler.fit_transform(well_train[['Azim', 'INCL','GR', 'Resistivity', 'DT', 'DTS', 'RHOB']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to dataframe\n",
    "#well_train_norm = pd.DataFrame(well_train_norm, columns=colnames)\n",
    "#well_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train.OS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = well_train['WELL'].astype(str)\n",
    "z = well_train['MD'].astype(float)\n",
    "os = well_train['OS'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = well_train['WELL'].astype(str)\n",
    "y= pd.DataFrame(y, columns=['WELL'])\n",
    "y= y.reset_index()\n",
    "y.drop('index', axis=1, inplace=True)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = well_train['MD'].astype(float)\n",
    "z= pd.DataFrame(z, columns=['MD'])\n",
    "z= z.reset_index()\n",
    "z.drop('index', axis=1, inplace=True)\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os = well_train['OS'].astype(int)\n",
    "os= pd.DataFrame(os, columns=['OS'])\n",
    "os= os.reset_index()\n",
    "os.drop('index', axis=1, inplace=True)\n",
    "os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "scaler= scaler.fit(well_train[only_feature])\n",
    "\n",
    "well_train_norm= scaler.transform(well_train[only_feature])\n",
    "well_train_norm = pd.DataFrame(well_train_norm, columns=only_feature)\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle.dump(scaler, open('Prob_pre_scaler_.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize using power transform Yeo-Johnson method\n",
    "#scaler = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "## ColumnTransformer\n",
    "#column_drop = ['WELL', 'Depth m']\n",
    "#ct = ColumnTransformer([('transform', scaler, only_feature)], remainder='passthrough')\n",
    "\n",
    "## fit and transform\n",
    "#well_train_norm = ct.fit_transform(well_train)\n",
    "\n",
    "## convert to dataframe\n",
    "#well_train_norm = pd.DataFrame(well_train_norm, columns=colnames)\n",
    "#well_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_norm = pd.concat([well_train_norm, y, z, os], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_norm['OS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalization\n",
    "# x = well_train_norm[feature_target].astype(float)\n",
    "sns.pairplot(well_train_norm, vars=feature_target, diag_kind = 'kde',\n",
    "             plot_kws = {'alpha': 0.6, 's': 30, 'edgecolor': 'k'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of well_train_norm, called well_train_dropped\n",
    "well_train_drop = well_train_norm.copy()\n",
    "\n",
    "# on the well_train_drop, drop WELL and DEPTH column\n",
    "well_train_drop = well_train_norm.drop(['WELL', 'MD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Standard Deviation Method (traditional)\n",
    "well_train_std = well_train_drop[np.abs(well_train_drop - well_train_drop.mean()) <= (3 * well_train_drop.std())]\n",
    "\n",
    "## delete all rows that have NaNs\n",
    "well_train_std = well_train_std.dropna()\n",
    "\n",
    "# Method 2: Isolation Forest\n",
    "iso = IsolationForest(contamination=0.5)\n",
    "yhat = iso.fit_predict(well_train_drop)\n",
    "mask = yhat != -1\n",
    "well_train_iso = well_train_norm[mask]\n",
    "\n",
    "# Method 3: Minimum Covariance Determinant\n",
    "ee = EllipticEnvelope(contamination=0.1)\n",
    "yhat = ee.fit_predict(well_train_drop)\n",
    "mask = yhat != -1\n",
    "well_train_ee = well_train_norm[mask]\n",
    "\n",
    "# Method 4: Local Outlier Factor\n",
    "lof = LocalOutlierFactor(contamination=0.3)\n",
    "yhat = lof.fit_predict(well_train_drop)\n",
    "mask = yhat != -1\n",
    "well_train_lof = well_train_norm[mask]\n",
    "\n",
    "# Method 5: One-class SVM\n",
    "svm = OneClassSVM(nu=0.1)\n",
    "yhat = svm.fit_predict(well_train_drop)\n",
    "mask = yhat != -1\n",
    "well_train_svm = well_train_norm[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of points before outliers removed                       :', len(well_train_norm))\n",
    "print('Number of points after outliers removed with Standard Deviation:', len(well_train_std))\n",
    "print('Number of points after outliers removed with Isolation Forest  :', len(well_train_iso))\n",
    "print('Number of points after outliers removed with Min. Covariance   :', len(well_train_ee))\n",
    "print('Number of points after outliers removed with Outlier Factor    :', len(well_train_lof))\n",
    "print('Number of points after outliers removed with One-class SVM     :', len(well_train_svm))\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "well_train_norm[feature_target].boxplot()\n",
    "plt.title('Before Outlier Removal', size=15)\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "well_train_std[feature_target].boxplot()\n",
    "plt.title('After Outlier Removal with Standard Deviation Filter', size=15)\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "well_train_iso[feature_target].boxplot()\n",
    "plt.title('After Outlier Removal with Isolation Forest', size=15)\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "well_train_ee[feature_target].boxplot()\n",
    "plt.title('After Outlier Removal with Min. Covariance', size=15)\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "well_train_lof[feature_target].boxplot()\n",
    "plt.title('After Outlier Removal with Local Outlier Factor', size=15)\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "well_train_svm[feature_target].boxplot()\n",
    "plt.title('After Outlier Removal with One-class SVM', size=15)\n",
    "\n",
    "plt.tight_layout(1.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_train_lof['OS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After outlier removal\n",
    "sns.pairplot(well_train_lof, vars=feature_target,\n",
    "             diag_kind='kde',\n",
    "             plot_kws = {'alpha': 0.6, 's': 30, 'edgecolor': 'k'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data (static)\n",
    "well_train_lof.plot(subplots=True, figsize=(15,35))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot\n",
    "# Converting the columns into numpy arrays \n",
    "depth = df.index\n",
    "MD = df['MD'].values \n",
    "DT = df['DT'].values \n",
    "DTS = df['DTS'].values \n",
    "RHOB = df['RHOB'].values \n",
    "Resistivity = df['Resistivity'].values \n",
    "gamma_ray = df['GR'].values \n",
    "\n",
    "\n",
    "# making 7 column and only 1 row \n",
    "fig = make_subplots(rows=1, cols=5)\n",
    "\n",
    "# plotting the graph of Gamma ray against depth \n",
    "fig.append_trace(go.Scatter(\n",
    "    x=gamma_ray,\n",
    "    y=depth,\n",
    "    name='Gamma-Ray',\n",
    "), row=1, col=1)\n",
    "# plotting the graph of shale volume against depth \n",
    "fig.append_trace(go.Scatter(\n",
    "    x=Resistivity,\n",
    "    y=depth,\n",
    "    name='Resistivity',\n",
    "), row=1, col=2)\n",
    "# plotting the graph of resitivity against depth \n",
    "fig.append_trace(go.Scatter(\n",
    "    x=RHOB,\n",
    "    y=depth, \n",
    "    name='Bulk density', \n",
    "), row=1, col=3)\n",
    "# plotting the graph of temperature against depth \n",
    "fig.append_trace(go.Scatter(\n",
    "    x=DTS, \n",
    "    y=depth,\n",
    "    name='Shear slowness', \n",
    "), row=1, col=4)\n",
    "# plotting the graph of velocity against depth \n",
    "fig.append_trace(go.Scatter(\n",
    "    x=DT, \n",
    "    y=depth,\n",
    "    name='Compressional slowness', \n",
    "), row=1, col=5)\n",
    "\n",
    "\n",
    "# showing the plots in a horizontal order\n",
    "fig.update_layout(height=2000, width=1300, title_text=\"Well Log Exploratory Data Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>Basic statistics</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(17,8))\n",
    "ax = plt.axes()\n",
    "sns.heatmap(well_train_lof.corr(), ax = ax, cmap='coolwarm', annot=True, linewidths=4, linecolor='black')\n",
    "ax.set_title(f'''Heatmap of the dataset''')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of the various duo_relationns in the data\n",
    "sns.pairplot(well_train_lof, hue='OS', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualization\n",
    "#import plotly.express as px\n",
    "#fig = px.scatter(df, x=\"Shale_Volume\", y=\"Restivity\", color=\"Classification\")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the count for non Deviated hole \n",
    "#majority_class = df.loc[df['OS'] == 0].count()[0]\n",
    "\n",
    "# Showing the count for Deviated hole \n",
    "#minority_class = df.loc[df['OS'] == 1].count()[0]\n",
    "\n",
    "# Printing the classes for the deviated and non-deviated class \n",
    "#print('Non Deviated Class (Classification = 0): {}'.format(majority_class))\n",
    "#print('Deviated Class (Classification = 1) : {}'.format(minority_class))\n",
    "\n",
    "#plt.figure(figsize=(9, 7))\n",
    "#sns.countplot(x=\"Classification\", data=df)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalibrate the samples with SMOTE\n",
    "# pip install imbalanced-learn\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#X = df.drop('Classification',axis=1).values\n",
    "#y = df[['Classification']].values.ravel()\n",
    "# Using SMOTE to Balance the imbalanced data \n",
    "#X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# convert y_resampled to df\n",
    "#df_y_resampled = pd.DataFrame(y_resampled,columns=['Classification'])\n",
    "\n",
    "# showing a plot of the Balanced dataset \n",
    "#majority_class = df_y_resampled.loc[df_y_resampled['Classification'] == 0].count()[0]\n",
    "\n",
    "# Showing the count for Non Hole Deviation \n",
    "#minority_class = df_y_resampled.loc[df_y_resampled['Classification'] == 1].count()[0]\n",
    "\n",
    "# Printing the classes for the deviated and non-deviated class \n",
    "#print('Non Deviated Class (Classification = 0): {}'.format(majority_class))\n",
    "#print('Deviated Class (Classification = 1) : {}'.format(minority_class))\n",
    "\n",
    "#plt.figure(figsize=(9, 7))\n",
    "\n",
    "#sns.countplot(x=\"Classification\", data=df_y_resampled)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Machine learning models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= well_train_lof.reset_index()\n",
    "data.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Azim', 'INCL','GR', 'Resistivity', 'DT', 'DTS', 'RHOB']\n",
    "target = 'OS'\n",
    "\n",
    "#well_train_lof\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n",
    "                                          test_size=0.3, \n",
    "                                          random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.OS))\n",
    "\n",
    "# Create the dataframes\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'OS']\n",
    "\n",
    "X_test  = data.loc[test_idx, feature_cols]\n",
    "y_test  = data.loc[test_idx, 'OS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data ain't balanced so during metric assassemnt we have to take that into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without principal component analysis\n",
    "n_components = 5\n",
    "pipelines = []\n",
    "n_estimators = 200\n",
    "seed = 42\n",
    "\n",
    "#print(df.shape)\n",
    "pipelines.append( ('SVC',\n",
    "                   Pipeline([\n",
    "                              #('sc', StandardScaler()),\n",
    "#                               ('pca', PCA(n_components = n_components, random_state=seed ) ),\n",
    "                             ('SVC', SVC(random_state=seed))]) ) )\n",
    "\n",
    "\n",
    "pipelines.append(('KNN',\n",
    "                  Pipeline([ \n",
    "                              #('sc', StandardScaler()),\n",
    "#                             ('pca', PCA(n_components = n_components, random_state=seed ) ),\n",
    "                            ('KNN', KNeighborsClassifier()) ])))\n",
    "pipelines.append( ('RF',\n",
    "                   Pipeline([\n",
    "                              #('sc', StandardScaler()),\n",
    "#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('RF', RandomForestClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\n",
    "\n",
    "\n",
    "pipelines.append( ('Ada',\n",
    "                   Pipeline([ \n",
    "                              #('sc', StandardScaler()),\n",
    "#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                    ('Ada', AdaBoostClassifier(random_state=seed,  n_estimators=n_estimators)) ]) ))\n",
    "\n",
    "pipelines.append( ('ET',\n",
    "                   Pipeline([\n",
    "                              #('sc', StandardScaler()),\n",
    "#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('ET', ExtraTreesClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\n",
    "pipelines.append( ('GB',\n",
    "                   Pipeline([ \n",
    "                             #('sc', StandardScaler()),\n",
    "#                             ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('GB', GradientBoostingClassifier(random_state=seed)) ]) ))\n",
    "\n",
    "pipelines.append( ('LR',\n",
    "                   Pipeline([\n",
    "                              #('sc', StandardScaler()),\n",
    "#                               ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('LR', LogisticRegression(random_state=seed)) ]) ))\n",
    "\n",
    "results, names, times  = [], [] , []\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in pipelines:\n",
    "    start = time()\n",
    "    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring = scoring,\n",
    "                                n_jobs=-1) \n",
    "    t_elapsed = time() - start\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    times.append(t_elapsed)\n",
    "    msg = \"%s: %f (+/- %f) performed in %f seconds\" % (name, 100*cv_results.mean(), \n",
    "                                                       100*cv_results.std(), t_elapsed)\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))    \n",
    "fig.suptitle(\"Algorithms comparison\")\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with a Random forest classifier\n",
    "pipe_rfc = Pipeline([ \n",
    "                      ('scl', StandardScaler()), \n",
    "                    ('rfc', RandomForestClassifier(random_state=seed, n_jobs=-1) )])\n",
    "\n",
    "# Set the grid parameters\n",
    "param_grid_rfc =  [ {\n",
    "    'rfc__n_estimators': [100, 200,300,400], # number of estimators\n",
    "    #'rfc__criterion': ['gini', 'entropy'],   # Splitting criterion\n",
    "    'rfc__max_features':[0.05 , 0.1], # maximum features used at each split\n",
    "    'rfc__max_depth': [None, 5], # Max depth of the trees\n",
    "    'rfc__min_samples_split': [0.005, 0.01], # mininal samples in leafs\n",
    "    }]\n",
    "# Use 10 fold CV\n",
    "kfold = StratifiedKFold(n_splits=num_folds, random_state= seed, shuffle=True)\n",
    "grid_rfc = GridSearchCV(pipe_rfc, param_grid= param_grid_rfc, cv=kfold, scoring=scoring, verbose= 1, n_jobs=-1)\n",
    "\n",
    "#Fit the pipeline\n",
    "start = time()\n",
    "grid_rfc = grid_rfc.fit(X_train, y_train)\n",
    "end = time()\n",
    "\n",
    "print(\"RFC grid search took %.3f seconds\" %(end-start))\n",
    "\n",
    "# Best score and best parameters\n",
    "print('-------Best score----------')\n",
    "print(grid_rfc.best_score_ * 100.0)\n",
    "print('-------Best params----------')\n",
    "print(grid_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some utility functions to plot the learning & validation curves\n",
    "\n",
    "def plot_learning_curve(train_sizes, train_scores, test_scores, title, alpha=0.1):\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plt.plot(train_sizes, train_mean, label='train score', color='blue', marker='o')\n",
    "    plt.fill_between(train_sizes,train_mean + train_std,\n",
    "                    train_mean - train_std, color='blue', alpha=alpha)\n",
    "    plt.plot(train_sizes, test_mean, label='test score', color='red',marker='o')\n",
    "    plt.fill_between(train_sizes,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Number of training points')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(ls='--')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    \n",
    "    \n",
    "def plot_validation_curve(param_range, train_scores, test_scores, title, alpha=0.1):\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plt.plot(param_range, train_mean, label='train score', color='blue', marker='o')\n",
    "    plt.fill_between(param_range,train_mean + train_std,\n",
    "                    train_mean - train_std, color='blue', alpha=alpha)\n",
    "    plt.plot(param_range, test_mean, label='test score', color='red', marker='o')\n",
    "    plt.fill_between(param_range,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n",
    "    plt.title(title)\n",
    "    plt.grid(ls='--')\n",
    "    plt.xlabel('Parameter value')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "              estimator= grid_rfc.best_estimator_ , X= X_train, y = y_train, \n",
    "                train_sizes=np.arange(0.1,1.1,0.1), cv= 10,  scoring='accuracy', n_jobs= - 1)\n",
    "\n",
    "plot_learning_curve(train_sizes, train_scores, test_scores, title='Learning curve for RFC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_clf=RandomForestClassifier(random_state=seed,\n",
    "                              max_features= 0.05, \n",
    "                              min_samples_split= 0.005, \n",
    "                              n_estimators= 300)\n",
    "\n",
    "RF_clf=RF_clf.fit(X_train, y_train)\n",
    "y_pred=RF_clf.predict(X_test)\n",
    "\n",
    "# final model evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the confusion matrix and \n",
    "print('confusion matrix')\n",
    "print('-'*30)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('')\n",
    "# print classification report\n",
    "print('classification report')\n",
    "print('-'*30)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "#for lab in coeff_labels:\n",
    "\n",
    "# Precision, recall, f-score from the multi-class support function\n",
    "precision, recall, fscore, _ = score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# The usual way to calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# ROC-AUC scores can be calculated by binarizing the data\n",
    "#auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n",
    "#          label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n",
    "#          average='weighted')\n",
    "\n",
    "# Last, the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                          'fscore':fscore, 'accuracy':accuracy}, name='Metrics'))\n",
    "#                          'auc':auc}))\n",
    "#                         name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "#fig, axList = plt.subplots(nrows=2, ncols=2)\n",
    "#axList = axList.flatten()\n",
    "#fig.set_size_inches(12, 10)\n",
    "\n",
    "#axList[-1].axis('off')\n",
    "\n",
    "#for ax,lab in zip(axList[:-1], coeff_labels):\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d');\n",
    "ax.set(title='Confusion metrics');\n",
    "    \n",
    "plt.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Visualization for the predicted problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Files \n",
    "D_files=[]\n",
    "Csv_file_list = []\n",
    "path = \"C:/Users/DELL/Desktop/Volve dump base data xml/Web_app_models/Problem forecast/Geo_log_prob_NW_data/Spec_data/\"\n",
    "# View contents of the path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Files\n",
    "Csv_file_list = []\n",
    "\n",
    "path = 'C:/Users/DELL/Desktop/Volve dump base data xml/Web_app_models/Problem forecast/Geo_log_prob_NW_data/Spec_data/'\n",
    "# View contents of the path\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Csv_file_list=[]\n",
    "for file in files:\n",
    "    if file.lower().endswith('.csv'):\n",
    "        Csv_file_list.append(path + file)\n",
    "        \n",
    "Csv_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two data set one as a seperate list\n",
    "# The second is a concatenated version\n",
    "# Renaminig the columns    \n",
    "\n",
    "df_list=[]\n",
    "for file in Csv_file_list:\n",
    "    df = pd.read_csv(file, index_col=None)\n",
    "    df_list.append(df)\n",
    "    \n",
    "well1, well2, well3, well4, well5= df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(Csv_file_list)):\n",
    "        print(f'''Example of the dataset {files[i][:-4]}''')\n",
    "        display(df_list[i].describe())\n",
    "        display(df_list[i].head())\n",
    "        display(df_list[i].info())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D for the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_well_path=well4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive well path visualization\n",
    "# Define df format\n",
    "\n",
    "#function to create wellpath plot\n",
    "def well_problem_plot(df):\n",
    "    x=df['S'].max()-df['S'].min()\n",
    "    y=df['W'].max()-df['W'].min()\n",
    "    z=df['TVD'].max()-df['TVD'].min()\n",
    "    \n",
    "    trace0 = go.Scatter3d(\n",
    "        x=df['S'],\n",
    "        y=df['W'],\n",
    "        z=df['TVD'],\n",
    "        mode='lines',\n",
    "        line=dict(\n",
    "            color='rgb(211,211,211)',\n",
    "            width=20\n",
    "        ),\n",
    "        hoverinfo='none',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "# Deviding the different plots accordingly to the severity of the dogleg\n",
    "# There are 6 sections \n",
    "\n",
    "    trace1 = go.Scatter3d(\n",
    "        x=df[df['OS']==0]['S'],\n",
    "        y=df[df['OS']==0]['W'],\n",
    "        z=df[df['OS']==0]['TVD'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='rgb(255, 174, 143)',\n",
    "            size=5\n",
    "        ),\n",
    "        name='Normal condition',\n",
    "        text = [\n",
    "                \"<b>TVD:</b> {} m<br>\"\n",
    "                \"<b>MD:</b> {} m<br>\"\n",
    "                \"<b>Inclination:</b> {} <br>\"\n",
    "                \"<b>Azimuth:</b> {} <br>\"\n",
    "\n",
    "                \"<b>UCS:</b> {} MPa<br>\"\n",
    "                \"<b>SH_MAX:</b> {} psi<br>\"\n",
    "                \"<b>SH_MIN:</b> {} psi<br>\"\n",
    "                \"<b>Static Poissons ratio:</b> {} <br>\"\n",
    "                \"<b>Static Young modulus:</b> {} Mpsi<br>\"\n",
    "                \"<b>Pore pressure:</b> {} psi<br>\"\n",
    "                \"<b>Vertical stress:</b> {} psi<br>\"\n",
    "            \n",
    "                .format(\n",
    "                        df['TVD'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['INCL'].loc[i],\n",
    "                        df['Azim'].loc[i],\n",
    "\n",
    "                        df['UCS'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['SHMAX_PHS'].loc[i],\n",
    "                        df['SHMIN_PHS'].loc[i],\n",
    "                        df['PR_STA'].loc[i],\n",
    "                        df['YME_STA'].loc[i],\n",
    "                        df['PPRS_HP_D'].loc[i],\n",
    "                        df['SVERTICAL_EXT'].loc[i])\n",
    "            \n",
    "                for i in df[df['OS']==0].index],\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    trace2 = go.Scatter3d(\n",
    "        x=df[df['OS']==1]['S'],\n",
    "        y=df[df['OS']==1]['W'],\n",
    "        z=df[df['OS']==1]['TVD'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='rgb(255, 103, 125)',\n",
    "            size=5\n",
    "        ),\n",
    "        name='Lost circulation',\n",
    "        text = [\n",
    "                \"<b>TVD:</b> {} m<br>\"\n",
    "                \"<b>MD:</b> {} m<br>\"\n",
    "                \"<b>Inclination:</b> {} <br>\"\n",
    "                \"<b>Azimuth:</b> {} <br>\"\n",
    "\n",
    "                \"<b>UCS:</b> {} MPa<br>\"\n",
    "                \"<b>SH_MAX:</b> {} psi<br>\"\n",
    "                \"<b>SH_MIN:</b> {} psi<br>\"\n",
    "                \"<b>Static Poissons ratio:</b> {} <br>\"\n",
    "                \"<b>Static Young modulus:</b> {} Mpsi<br>\"\n",
    "                \"<b>Pore pressure:</b> {} psi<br>\"\n",
    "                \"<b>Vertical stress:</b> {} psi<br>\"\n",
    "            \n",
    "                .format(\n",
    "                        df['TVD'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['INCL'].loc[i],\n",
    "                        df['Azim'].loc[i],\n",
    "\n",
    "                        df['UCS'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['SHMAX_PHS'].loc[i],\n",
    "                        df['SHMIN_PHS'].loc[i],\n",
    "                        df['PR_STA'].loc[i],\n",
    "                        df['YME_STA'].loc[i],\n",
    "                        df['PPRS_HP_D'].loc[i],\n",
    "                        df['SVERTICAL_EXT'].loc[i])\n",
    "            \n",
    "                for i in df[df['OS']==1].index],\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    trace3 = go.Scatter3d(\n",
    "        x=df[df['OS']==2]['S'],\n",
    "        y=df[df['OS']==2]['W'],\n",
    "        z=df[df['OS']==2]['TVD'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='rgb(111, 90, 126)',\n",
    "            size=5\n",
    "        ),\n",
    "        name='Stuck Bottom Hole Assembley',\n",
    "        text = [\n",
    "                \"<b>TVD:</b> {} m<br>\"\n",
    "                \"<b>MD:</b> {} m<br>\"\n",
    "                \"<b>Inclination:</b> {} <br>\"\n",
    "                \"<b>Azimuth:</b> {} <br>\"\n",
    "\n",
    "                \"<b>UCS:</b> {} MPa<br>\"\n",
    "                \"<b>SH_MAX:</b> {} psi<br>\"\n",
    "                \"<b>SH_MIN:</b> {} psi<br>\"\n",
    "                \"<b>Static Poissons ratio:</b> {} <br>\"\n",
    "                \"<b>Static Young modulus:</b> {} Mpsi<br>\"\n",
    "                \"<b>Pore pressure:</b> {} psi<br>\"\n",
    "                \"<b>Vertical stress:</b> {} psi<br>\"\n",
    "            \n",
    "                .format(\n",
    "                        df['TVD'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['INCL'].loc[i],\n",
    "                        df['Azim'].loc[i],\n",
    "\n",
    "                        df['UCS'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['SHMAX_PHS'].loc[i],\n",
    "                        df['SHMIN_PHS'].loc[i],\n",
    "                        df['PR_STA'].loc[i],\n",
    "                        df['YME_STA'].loc[i],\n",
    "                        df['PPRS_HP_D'].loc[i],\n",
    "                        df['SVERTICAL_EXT'].loc[i])\n",
    "            \n",
    "                for i in df[df['OS']==2].index],\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    trace4 = go.Scatter3d(\n",
    "        x=df[df['OS']==3]['S'],\n",
    "        y=df[df['OS']==3]['W'],\n",
    "        z=df[df['OS']==3]['TVD'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='rgb(206, 18, 18)',\n",
    "            size=5\n",
    "        ),\n",
    "        name='Equipement failure',\n",
    "        text = [\n",
    "                \"<b>TVD:</b> {} m<br>\"\n",
    "                \"<b>MD:</b> {} m<br>\"\n",
    "                \"<b>Inclination:</b> {} <br>\"\n",
    "                \"<b>Azimuth:</b> {} <br>\"\n",
    "\n",
    "                \"<b>UCS:</b> {} MPa<br>\"\n",
    "                \"<b>SH_MAX:</b> {} psi<br>\"\n",
    "                \"<b>SH_MIN:</b> {} psi<br>\"\n",
    "                \"<b>Static Poissons ratio:</b> {} <br>\"\n",
    "                \"<b>Static Young modulus:</b> {} Mpsi<br>\"\n",
    "                \"<b>Pore pressure:</b> {} psi<br>\"\n",
    "                \"<b>Vertical stress:</b> {} psi<br>\"\n",
    "            \n",
    "                .format(\n",
    "                        df['TVD'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['INCL'].loc[i],\n",
    "                        df['Azim'].loc[i],\n",
    "\n",
    "                        df['UCS'].loc[i],\n",
    "                        df['DEPT'].loc[i],\n",
    "                        df['SHMAX_PHS'].loc[i],\n",
    "                        df['SHMIN_PHS'].loc[i],\n",
    "                        df['PR_STA'].loc[i],\n",
    "                        df['YME_STA'].loc[i],\n",
    "                        df['PPRS_HP_D'].loc[i],\n",
    "                        df['SVERTICAL_EXT'].loc[i])\n",
    "            \n",
    "                for i in df[df['OS']==3].index],\n",
    "        hoverinfo='text'\n",
    "    )\n",
    " \n",
    "    data = [trace0,trace1,trace2,trace3,trace4]\n",
    "\n",
    "    layout = dict(\n",
    "        width=900,\n",
    "        height=800,\n",
    "        margin=dict(t=0,b=0, pad=0),\n",
    "        autosize=False,\n",
    "        legend=dict(x=1,y=0.85),\n",
    "        hoverlabel=dict(\n",
    "                        bgcolor='rgb(255,255,255)',\n",
    "                        bordercolor='rgb(0,0,0)'\n",
    "                        ),\n",
    "        hovermode='closest',\n",
    "        scene=dict(\n",
    "            xaxis=dict(\n",
    "                title='<b>Northing (m)</b>',\n",
    "                gridcolor='rgb(169,169,169)',\n",
    "                zerolinecolor='rgb(169,169,169)',\n",
    "                showbackground=True,\n",
    "                backgroundcolor='rgb(200, 200, 230)',\n",
    "                showspikes=False\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title='<b>Easting (m)</b>',\n",
    "                gridcolor='rgb(169,169,169)',\n",
    "                zerolinecolor='rgb(169,169,169)',\n",
    "                showbackground=True,\n",
    "                backgroundcolor='rgb(230, 200,230)',\n",
    "                showspikes=False\n",
    "            ),\n",
    "            zaxis=dict(\n",
    "                title='<b>Depth (m)</b>',\n",
    "                gridcolor='rgb(169,169,169)',\n",
    "                zerolinecolor='rgb(169,169,169)',\n",
    "                showbackground=True,\n",
    "                backgroundcolor='rgb(230, 230,200)',\n",
    "                showspikes=False,\n",
    "                autorange='reversed'\n",
    "            ),\n",
    "            camera=dict(\n",
    "                up=dict(\n",
    "                    x=0,\n",
    "                    y=0,\n",
    "                    z=1\n",
    "                ),\n",
    "                eye=dict(\n",
    "                    x=-1.25,\n",
    "                    y=1.25,\n",
    "                    z=0.5,\n",
    "                )\n",
    "            ),\n",
    "            aspectratio = dict(x=x/z, y=y/z, z=1),\n",
    "            aspectmode = 'manual'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    figure = dict(data=data, layout=layout)\n",
    "    fig=go.Figure(data,layout)\n",
    "    return fig.show()\n",
    "\n",
    "\n",
    "display(well_problem_plot(df_well_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Saving the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'Drilling_problems_forecast.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Deep learning comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building An MLP \n",
    "model = Sequential() \n",
    "model.add(Dense(32, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# Model summary \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best: 0.9585470080375671 using {'batch_size': 5, 'unit1': 50, 'unit2': 40, 'unit3': 10}\n",
    "\n",
    "# Training the model \n",
    "hist = model.fit(X_train, y_train, epochs = 10, batch_size = 4, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulating the model accuracy  \n",
    "accuracy = model.evaluate(X_test, y_test)[1] * 100 \n",
    "accuracy = str(accuracy)[:5]\n",
    "!echo \n",
    "print('The model is {}% Accurate.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing a plot of how Accurate the model is against the \n",
    "# test dataset\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.grid(True)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing a plot of the loss with respect to the number of epochs \n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.grid(True)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
